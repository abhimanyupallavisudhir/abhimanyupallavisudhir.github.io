Well we can start experimenting in one of two set ups:
(1) You have Alice and Bob, and Bob swears to prove Alice wrong each time.
(2) You have just Bob, and Bob swears to prove the universe wrong each time.

The second sounds easier, but actually let's start with the first.

The state of the universe, w, tells you everything that will happen, including what Bob will do next round.

If we want to say Alice literally cannot assign a probability to Bob's actions next round, we need to say the sigma algebra does not contain "all sets where Bob chooses 0".

Of course, if Alice _didn't_ know that Bob was adverserial, she _could_ assign a probability to his actions.

Which means she can assign a probability to "Bob chooses left based on a coin flip AND he chooses left", but she cannot assign a probability to "Bob is a rebel AND he chooses left".

_If_ Bob is a rebel, Alice cannot know anything more precise about his actions. So you know, Alice's sigma algebra is generated by:

{<Bob is a coin-flipper, He chooses left, Potatoes are expensive>},
{<Bob is a coin-flipper, He chooses left, Potatoes are cheap>}
{<Bob is a coin-flipper, He chooses right, Potatoes are expensive>},
{<Bob is a coin-flipper, He chooses right, Potatoes are cheap>},
{<Bob is a rebel, He chooses left, Potatoes are expensive>, <Bob is a rebel, He chooses right, Potatoes are expensive>},
{<Bob is a rebel, He chooses left, Potatoes are cheap>, <Bob is a rebel, He chooses right, Potatoes are cheap>}

Problem is we can't just say -- oh, such and such is the sigma algebra. Because Alice might not even know what Bob does (by Rice's theorem) ... right?

Actually no -- if Alice doesn't know what Bob does, she can still assign probabilities to the possibilities ... right?

Yeah yeah -- because even if she can't assign probabilities to Bob's semantics, she can still assign probabilities to his syntactics!

But it's not like she can't have opinions on anyone's semantics. There are _some_ limits to how fine her sigma algebra can get, and we need to determine this.

First of all -- does perfect Bayes rationality mean she has the finest possible sigma algebra? No, right? Does information flow refine the sigma algebra?
I think I need to understand filtration. Ok, done.

Note that the filtration here is kinda subtle ... we're not saying that all of the sigma algebra at any point actually represents the information known to the agent -- it's all the information it _could_ assign probabilities to; it takes the role of the full sigma algebra in this sense.

OK, but how do we actually write down this sigma algebra in general.

Can we just say: exclude all undecidable statements? IDK. That seems a bit weird. 

What does Godel's second incompleteness theorem say here, actually?

When we hear the argument -- that Bob will lift his arm only if he sees Alice decides that has probability <= 0.8, we're like: if Alice gives it a probability p, it must actually happen with probability p. But actually if p <= 0.8, it happens with probability 1, and if p > 0.8 it happens with probability 0. Thus p does not exist.

Can Alice assign a probability to the statement "Anything I assign a probability of p to, happens with probability q"? Suppose she can. Then she reasons: If Alice gives it a probability p, it must actually happen with probability q. But actually if p <= 0.8, then it happens with probability 1, and if p > 0.8 it happens with probability 0. So q better be 1 for <= 0.8, 0 for > 0.8 ... ?

But that's not really fair, right? I mean, the calibration function is a statement about conditioning _only_ on the fact that Alice believes it. When you also condition on other things, it's different.

Alice wants to find P(Bob raises arm | all known facts).
All known facts = "Alice's P(Pob raises arm | all given facts) < 0.8  <=>  Bob raises arm", "Alice's P = ... "
    ---"P(X | Alice's P(X | all given facts) = p, ______) = p????" ---

Huh ... let's think about that again.

Alice defines some probability measure P on some sigma algebra on Omega.
She can calculate conditional probabilities like P(Bob | i, x) ... 
Since she can have a belief about the state of the world -- in particular, she can have a belief about what her belief is.
To "have a belief on something" means that that thing (a RV) is measurable wrt your sigma algebra.
So there's a function P : Omega -> ~P~ , the RV representing Alice's beliefs, and for Alice to have a belief means that the preimages of each singleton in ~P~, every possible belief system Alice could have in that turn, is in Alice's sigma-algebra. This means Alice can assign a probability to "Alice assigns a probability of bla to bla".

So Alice can reason: Whatever probability I assign to Bob halting will be wrong, so I must not be able to assign a probability to Bob halting, so the probability of Bob halting is 0, which means I assign the probability of 0 to Bob halting, which means contradiction.

What this means is: Charles is made aware of -- i contains -- the following:
- Bob will halt if Alice's P assigned to it is <= 0.8, or if Alice does not assign a P to it [eliminate states wherein P is something and Bob does not act in such way]
- Alice's soundness ... 

What, exactly, is Alice's soundness?
What is a state that implies an unsound Alice? 
Actually, I think we can think about both soundness and consistency.
- Because sure, the straightforward way to define soundness is something like-- Alice assigns the correct probability to things.
- The way to define consistency is that ... not all states are eliminated ... I think??

CAN a Bayes-rational Alice assign a probability to whether Bob halts or not? What property of Alice means she cannot assign a probability to Bob's halting? 

Suppose "Bob halts" is a measurable set in Alice's sigma-algebra. 
Suppose that Alice is aware of the probabilities it assigns to things -- i.e. there is an A-measurable map from Omega to the set of all possible measures (HUH?)
Suppose Alice is informed of Bob's programming.
Then Alice eliminates all states where her probability assignment is >= 0.8 and Bob halts, and all states where her probability assignment is < 0.8 and Bob doesn't halt.
Alice eliminates ... aah, I don't know.

I feel like this whole thing needs a fresh approach. I mean, it's not even possible that Alice assigns a probability to every possible sigma algebra she could have ...

1) Do some kind of an "inverse limit" construction to bypass Cantor ...

2) Is there such a thing as a "maximal sigma algebra"? Maybe it's more like if this set is included in the sigma algebra, then this isn't ... Is there a rule like the "join F U G of ok sigma algebras is also ok"?




---------





Let's make the most basic possible construction.

We have a universe with exactly one agent alpha, and this agent tries to assign have opinions about as much stuff it can.

All alpha can do is assign a probability measure on Omega.

And Omega consists of all the possible probability measures that alpha could have.

Well first of all there's obviously a cardinality issue. Also if you allow the agent to be able to do things, then you can get it to violate its own programming and get a Godelian issue.

Mathematically what we "want" to say is: 
{P} = [0, 1]^Omega (-ish -- the boundary of that actually)
Omega = {P}
... that alpha assigns a probability to each possible probability measure it can have...
... or to some partition of possible probability measures. And to get a natural such partition is achieved with inverse limits ... (?)


