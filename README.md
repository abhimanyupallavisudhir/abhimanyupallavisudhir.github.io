<!---
## Research

My goal is to design an AI agent whose internal structure is that of a market (because markets and intelligent agents do basically the same things; markets : intelligence :: capitalism : learning). My research will prevent the now-seemingly-inevitable destruction of the world and lead us to a glorious utopia, for three broad reasons:

* **(Alignment is Industrial Organization)** Markets serve the consumers, they don't develop nefarious goals of their own, at least when you have a decent antitrust system, or spies. In fact this also addresses the question of how to handle conflicting interests between the humans we'd like to align the AI to.
* **(Eliciting Latent Knowledge = Interpretability)** We could maybe get agents to bet on questions on their own latent space, or get agents to somehow bet on each others' latent spaces in a useful manner.
* **(Coherent Extrapolated Volition)** I like transhumanism as much as the next android, but there was a Jimmy Neutron episode wherein one of the characters' heads grew 500x its size and he became evil. It's not obvious how to factor someone as Intelligence X Utility_function, because any "utility function" you might have is going to be some rationalization/as-if fit to the person's actions, which are limited by his bounded rationality. But markets solve the question of bounded rationality; "more algorithmic information" just means "more traders".

So far all I've done is devise prediction markets for First-Order Logic (and also hyperarithmetical) sentences -- preprint out soon. Some side-quests that have emerged as a corollary of all this -- contact if you're interested in collaborating, because I don't have the time to do them all:

* Building practical markets: I can prove that a very theoretical implication of my framework works; designing real markets for this will require some work.
* Implications for mathematical logic: What if you let mathematical theories bet on markets? If they make a lot of money, we should trust them. Maybe this gives us a nice way to think about proof-theoretic ordinals etc. IDK.
* Rewriting probability theory: I think the probability theory axioms are wrong and should be rewritten in a way to make long-term prices on my market a probability distribution. In particular, a sigma-algebra should only be required to be closed under countable union of a computable enumeration of its elements.
* Bridges to neural networks: As it stands, program markets are absurdly impractical. Now I think I can solve this with latent spaces, but maybe we can somehow transfer our ideas to neural networks, or interpret neural networks as markets.

The first one will help you get rich; the second and third will satiate your deepest intellectual curiosities; the last one will prevent the world from being destroyed.

<table>
  <tbody>
    <tr>
      <th></th>
      <th>Links</th>
    </tr>
    <tr>
      <td><img src="face.jpg" width="200"></td>
      <td>
        <ul>
          <li><a href = "https://warwick.ac.uk/fac/sci/dcs/people/u2251609/">PhD student @ University of Warwick</a> </li> 
          <li>CV: <a href="https://drive.google.com/file/d/1_SfMm_qtLpzms3agqb6zf-76-Vq7U5VZ/view?usp=drive_link">relevant</a>, <a href ="https://drive.google.com/file/d/1_VStBkw6DthYNyp4F3Xx2THInWLpNrHK/view?usp=drive_link">extended</a> </li> 
          <li>Blogs: <a href="https://thewindingnumber.blogspot.com">The Winding Number</a>, <a href="https://copypasta.substack.com/">Copypasta</a></li> 
          <li>Profiles: <a href="https://www.lesswrong.com/users/abhimanyu-pallavi-sudhir">LessWrong</a>, <a href="https://math.stackexchange.com/users/78451/abhimanyu-pallavi-sudhir">Stack Exchange</a>, <a href="https://twitter.com/abhimanyupasu">X (Twitter)</a></li>
          <li>Also profiles: <a href="https://www.linkedin.com/in/abhimanyu-pallavi-sudhir/">LinkedIn</a>, <a href="https://scholar.google.com/citations?user=lb38BjYAAAAJ&hl=en">GScholar</a>, <a href="https://orcid.org/0000-0002-2506-0515">ORCID</a> </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
--->

<table>
  <tbody>
    <tr>
      <th></th>
      <th>Research</th>
    </tr>
    <tr>
      <td valign="top">
        <img src="face.jpg" width="1000">
        <ul>
          <li><a href = "https://warwick.ac.uk/fac/sci/dcs/people/u2251609/">PhD student @ University of Warwick</a> </li> 
          <li>CV: <a href="https://drive.google.com/file/d/1_SfMm_qtLpzms3agqb6zf-76-Vq7U5VZ/view?usp=drive_link">relevant</a>, <a href ="https://drive.google.com/file/d/1_VStBkw6DthYNyp4F3Xx2THInWLpNrHK/view?usp=drive_link">extended</a> </li> 
          <li>Blogs: <a href="https://thewindingnumber.blogspot.com">The Winding Number</a>, <a href="https://copypasta.substack.com/">Copypasta</a></li> 
          <li>Profiles: <a href="https://www.lesswrong.com/users/abhimanyu-pallavi-sudhir">LessWrong</a>, <a href="https://math.stackexchange.com/users/78451/abhimanyu-pallavi-sudhir">Stack Exchange</a>, <a href="https://twitter.com/abhimanyupasu">X (Twitter)</a></li>
          <li>Also profiles: <a href="https://www.linkedin.com/in/abhimanyu-pallavi-sudhir/">LinkedIn</a>, <a href="https://scholar.google.com/citations?user=lb38BjYAAAAJ&hl=en">GScholar</a>, <a href="https://orcid.org/0000-0002-2506-0515">ORCID</a> </li>
        </ul>
      </td>
      <td>
My goal is to design an AI agent whose internal structure is that of a market (because markets and intelligent agents do basically the same things; markets : intelligence :: capitalism : learning). My research will prevent the now-seemingly-inevitable destruction of the world and lead us to a glorious utopia, for three broad reasons:

* **(Alignment is Industrial Organization)** Markets serve the consumers, they don't develop nefarious goals of their own, at least when you have a decent antitrust system, or spies. In fact this also addresses the question of how to handle conflicting interests between the humans we'd like to align the AI to.
* **(Eliciting Latent Knowledge = Interpretability)** We could maybe get agents to bet on questions on their own latent space, or get agents to somehow bet on each others' latent spaces in a useful manner.
* **(Coherent Extrapolated Volition)** I like transhumanism as much as the next android, but there was a Jimmy Neutron episode wherein one of the characters' heads grew 500x its size and he became evil. It's not obvious how to factor someone as Intelligence X Utility_function, because any "utility function" you might have is going to be some rationalization/as-if fit to the person's actions, which are limited by his bounded rationality. But markets solve the question of bounded rationality; "more algorithmic information" just means "more traders".

So far all I've done is devise prediction markets for First-Order Logic (and also hyperarithmetical) sentences -- preprint out soon. Some side-quests that have emerged as a corollary of all this -- contact if you're interested in collaborating, because I don't have the time to do them all:

* Building practical markets: I can prove that a very theoretical implication of my framework works; designing real markets for this will require some work.
* Implications for mathematical logic: What if you let mathematical theories bet on markets? If they make a lot of money, we should trust them. Maybe this gives us a nice way to think about proof-theoretic ordinals etc. IDK.
* Rewriting probability theory: I think the probability theory axioms are wrong and should be rewritten in a way to make long-term prices on my market a probability distribution. In particular, a sigma-algebra should only be required to be closed under countable union of a computable enumeration of its elements.
* Bridges to neural networks: As it stands, program markets are absurdly impractical. Now I think I can solve this with latent spaces, but maybe we can somehow transfer our ideas to neural networks, or interpret neural networks as markets.

The first one will help you get rich; the second and third will satiate your deepest intellectual curiosities; the last one will prevent the world from being destroyed.
      </td>
    </tr>
  </tbody>
</table>
